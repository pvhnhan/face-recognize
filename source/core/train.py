"""
Training script cho h·ªá th·ªëng nh·∫≠n di·ªán khu√¥n m·∫∑t
S·ª≠ d·ª•ng YOLOv7 cho face detection v√† DeepFace cho face recognition
"""

import os
import sys
import logging
import time
import json
from pathlib import Path
from typing import Dict, List, Optional
import cv2
import numpy as np
import pandas as pd
from datetime import datetime
from deepface import DeepFace

# Th√™m ƒë∆∞·ªùng d·∫´n ƒë·ªÉ import c√°c module
sys.path.append(str(Path(__file__).parent.parent))

from utils.data_processor import DataProcessor
from utils.face_utils import FaceProcessor
from utils.image_utils import ImageProcessor
from config.config import TRAINING_CONFIG, MODEL_CONFIG, DATA_CONFIG

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=getattr(logging, TRAINING_CONFIG['logging']['level']),
    format=TRAINING_CONFIG['logging']['format'],
    handlers=[
        logging.FileHandler(TRAINING_CONFIG['logging']['file']),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class FaceRecognitionTrainer:
    """
    L·ªõp hu·∫•n luy·ªán h·ªá th·ªëng nh·∫≠n di·ªán khu√¥n m·∫∑t
    """
    
    def __init__(self):
        """Kh·ªüi t·∫°o trainer"""
        self.data_processor = DataProcessor()
        self.face_processor = FaceProcessor()
        self.image_processor = ImageProcessor()
        
        # ƒê∆∞·ªùng d·∫´n
        self.data_dir = Path(DATA_CONFIG['data_dir'])
        self.models_dir = Path(MODEL_CONFIG['models_dir'])
        self.yolo_dataset_dir = self.data_dir / 'yolo_dataset'
        self.embeddings_dir = self.models_dir / 'face_embeddings'
        
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        self.embeddings_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("Kh·ªüi t·∫°o Face Recognition Trainer")
    
    def prepare_dataset(self) -> bool:
        """
        Chu·∫©n b·ªã dataset cho training
        
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            logger.info("B·∫Øt ƒë·∫ßu chu·∫©n b·ªã dataset...")
            
            # Ki·ªÉm tra metadata
            metadata_path = self.data_dir / 'metadata.csv'
            if not metadata_path.exists():
                logger.error("Kh√¥ng t√¨m th·∫•y metadata.csv")
                return False
            
            # ƒê·ªçc metadata ƒë·ªÉ ki·ªÉm tra
            metadata_df = pd.read_csv(metadata_path)
            logger.info(f"ƒê·ªçc ƒë∆∞·ª£c {len(metadata_df)} records t·ª´ metadata")
            
            # Chu·∫©n b·ªã YOLO dataset (kh√¥ng c·∫ßn truy·ªÅn metadata_df)
            success = self.data_processor.prepare_yolo_dataset(
                output_dir=self.yolo_dataset_dir
            )
            
            if success:
                logger.info("Chu·∫©n b·ªã dataset th√†nh c√¥ng!")
                return True
            else:
                logger.error("Chu·∫©n b·ªã dataset th·∫•t b·∫°i!")
                return False
                
        except Exception as e:
            logger.error(f"L·ªói khi chu·∫©n b·ªã dataset: {e}")
            return False
    
    def create_embeddings(self) -> bool:
        """
        T·∫°o embeddings cho t·∫•t c·∫£ khu√¥n m·∫∑t trong dataset
        
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            logger.info("B·∫Øt ƒë·∫ßu t·∫°o embeddings...")
            
            # ƒê·ªçc metadata
            metadata_path = self.data_dir / 'metadata.csv'
            metadata_df = pd.read_csv(metadata_path)
            
            # T·∫°o embeddings mapping
            embeddings_mapping = {}
            total_faces = 0
            successful_faces = 0
            
            for idx, row in metadata_df.iterrows():
                try:
                    image_path = self.data_dir / 'raw_images' / row['filename']
                    if not image_path.exists():
                        logger.warning(f"Kh√¥ng t√¨m th·∫•y ·∫£nh: {image_path}")
                        continue
                    
                    # ƒê·ªçc ·∫£nh
                    image = cv2.imread(str(image_path))
                    if image is None:
                        logger.warning(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh: {image_path}")
                        continue
                    
                    # Ph√°t hi·ªán khu√¥n m·∫∑t s·ª≠ d·ª•ng DeepFace (m·∫°nh h∆°n OpenCV)
                    try:
                        # Chuy·ªÉn BGR sang RGB cho DeepFace
                        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                        
                        # S·ª≠ d·ª•ng DeepFace ƒë·ªÉ ph√°t hi·ªán khu√¥n m·∫∑t
                        face_objs = DeepFace.extract_faces(
                            img_path=image_rgb,
                            detector_backend='opencv',
                            enforce_detection=False
                        )
                        
                        if len(face_objs) == 0:
                            # Fallback v·ªÅ OpenCV n·∫øu DeepFace kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c
                            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                            faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                            
                            if len(faces) == 0:
                                logger.warning(f"Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t trong: {image_path}")
                                continue
                            
                            # L·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n
                            x, y, w, h = faces[0]
                            face_region = image[y:y+h, x:x+w]
                        else:
                            # S·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ DeepFace
                            face_obj = face_objs[0]
                            face_region = face_obj['face']
                            # Chuy·ªÉn v·ªÅ BGR ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code c≈©
                            face_region = cv2.cvtColor(face_region, cv2.COLOR_RGB2BGR)
                            # L·∫•y bbox t·ª´ DeepFace
                            facial_area = face_obj['facial_area']
                            x, y, w, h = facial_area['x'], facial_area['y'], facial_area['w'], facial_area['h']
                    except Exception as e:
                        logger.warning(f"L·ªói DeepFace detection, fallback v·ªÅ OpenCV: {e}")
                        # Fallback v·ªÅ OpenCV
                        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
                        
                        if len(faces) == 0:
                            logger.warning(f"Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t trong: {image_path}")
                            continue
                        
                        # L·∫•y khu√¥n m·∫∑t ƒë·∫ßu ti√™n
                        x, y, w, h = faces[0]
                        face_region = image[y:y+h, x:x+w]
                    
                    # Tr√≠ch xu·∫•t embedding
                    face_embedding = self.face_processor.extract_face_embedding(face_region)
                    
                    if face_embedding is not None:
                        # L∆∞u embedding
                        embedding_key = f"{row['employee_id']}_{row['filename']}"
                        embeddings_mapping[embedding_key] = {
                            'employee_id': row['employee_id'],
                            'full_name': row['full_name'],
                            'image_path': str(image_path),
                            'embedding': face_embedding.tolist(),
                            'bbox': [int(x), int(y), int(w), int(h)],
                            'created_at': datetime.now().isoformat()
                        }
                        successful_faces += 1
                        logger.info(f"T·∫°o embedding th√†nh c√¥ng cho: {row['full_name']} ({row['employee_id']})")
                    
                    total_faces += 1
                    
                except Exception as e:
                    logger.error(f"L·ªói khi x·ª≠ l√Ω ·∫£nh {row['filename']}: {e}")
                    continue
            
            # L∆∞u embeddings mapping
            embeddings_file = self.embeddings_dir / 'embeddings_mapping.json'
            with open(embeddings_file, 'w', encoding='utf-8') as f:
                json.dump(embeddings_mapping, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Ho√†n th√†nh t·∫°o embeddings: {successful_faces}/{total_faces} khu√¥n m·∫∑t th√†nh c√¥ng")
            logger.info(f"L∆∞u embeddings t·∫°i: {embeddings_file}")
            
            return successful_faces > 0
            
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o embeddings: {e}")
            return False
    
    def train_yolov7(self) -> bool:
        """
        Hu·∫•n luy·ªán YOLOv7 model cho face detection
        
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            logger.info("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán YOLOv7...")
            
            # Ki·ªÉm tra YOLOv7 repository
            yolov7_path = Path(__file__).parent.parent / 'yolov7'
            if not yolov7_path.exists():
                logger.error("Kh√¥ng t√¨m th·∫•y YOLOv7 repository")
                return False
            
            # Ki·ªÉm tra dataset
            if not self.yolo_dataset_dir.exists():
                logger.error("Kh√¥ng t√¨m th·∫•y YOLO dataset")
                return False
            
            # C√†i ƒë·∫∑t YOLOv7 dependencies n·∫øu c·∫ßn
            if not self._install_yolov7_dependencies(yolov7_path):
                logger.warning("Kh√¥ng th·ªÉ c√†i ƒë·∫∑t YOLOv7 dependencies, th·ª≠ training v·ªõi dependencies hi·ªán t·∫°i")
            
            # T·∫°o file config cho YOLOv7
            data_yaml = self._create_yolo_data_config()
            if not data_yaml:
                return False
            
            # Ki·ªÉm tra dataset tr∆∞·ªõc khi training
            if not self._validate_dataset():
                logger.error("Dataset validation th·∫•t b·∫°i")                                
            
            # Debug: Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n dataset
            logger.info(f"Dataset path: {self.yolo_dataset_dir}")
            logger.info(f"Dataset exists: {self.yolo_dataset_dir.exists()}")
            logger.info(f"Train images dir: {self.yolo_dataset_dir / 'images' / 'train'}")
            logger.info(f"Train images exists: {(self.yolo_dataset_dir / 'images' / 'train').exists()}")
            logger.info(f"Val images dir: {self.yolo_dataset_dir / 'images' / 'val'}")
            logger.info(f"Val images exists: {(self.yolo_dataset_dir / 'images' / 'val').exists()}")
            
            # Debug th√™m: Ki·ªÉm tra t·ª´ th∆∞ m·ª•c YOLOv7
            if yolov7_path.exists():
                logger.info(f"YOLOv7 path: {yolov7_path}")
                logger.info(f"YOLOv7 exists: {yolov7_path.exists()}")
                
                # Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ YOLOv7
                relative_dataset_path = yolov7_path.parent / "data" / "yolo_dataset"
                logger.info(f"Relative dataset path from YOLO: {relative_dataset_path}")
                logger.info(f"Relative dataset exists: {relative_dataset_path.exists()}")
                
                if relative_dataset_path.exists():
                    train_path_from_yolo = relative_dataset_path / "images" / "train"
                    val_path_from_yolo = relative_dataset_path / "images" / "val"
                    logger.info(f"Train path from YOLO: {train_path_from_yolo}")
                    logger.info(f"Train path from YOLO exists: {train_path_from_yolo.exists()}")
                    logger.info(f"Val path from YOLO: {val_path_from_yolo}")
                    logger.info(f"Val path from YOLO exists: {val_path_from_yolo.exists()}")
                    
                    if train_path_from_yolo.exists():
                        train_files = list(train_path_from_yolo.glob("*"))
                        logger.info(f"Train files count: {len(train_files)}")
                        if train_files:
                            logger.info(f"First few train files: {[f.name for f in train_files[:3]]}")
                    
                    if val_path_from_yolo.exists():
                        val_files = list(val_path_from_yolo.glob("*"))
                        logger.info(f"Val files count: {len(val_files)}")
                        if val_files:
                            logger.info(f"First few val files: {[f.name for f in val_files[:3]]}")
            
            # Ki·ªÉm tra n·ªôi dung th∆∞ m·ª•c
            if self.yolo_dataset_dir.exists():
                logger.info(f"Dataset contents: {list(self.yolo_dataset_dir.iterdir())}")
                if (self.yolo_dataset_dir / 'images').exists():
                    logger.info(f"Images contents: {list((self.yolo_dataset_dir / 'images').iterdir())}")
                    if (self.yolo_dataset_dir / 'images' / 'train').exists():
                        train_files = list((self.yolo_dataset_dir / 'images' / 'train').glob('*'))
                        logger.info(f"Train files: {len(train_files)} files")
                        if len(train_files) > 0:
                            logger.info(f"First few train files: {[f.name for f in train_files[:5]]}")
            
            # Ki·ªÉm tra th∆∞ m·ª•c hi·ªán t·∫°i v√† t·∫°o symlink n·∫øu c·∫ßn
            import os
            current_dir = os.getcwd()
            logger.info(f"Current working directory: {current_dir}")
            
            # T·∫°o symlink t·ª´ yolov7/data ƒë·∫øn dataset th·ª±c t·∫ø
            yolov7_data_dir = yolov7_path / 'data'
            if not yolov7_data_dir.exists():
                yolov7_data_dir.mkdir(parents=True, exist_ok=True)
            

            
            # T·∫°o symlink cho dataset
            dataset_symlink = yolov7_data_dir / 'yolo_dataset'
            if dataset_symlink.exists():
                if dataset_symlink.is_symlink():
                    dataset_symlink.unlink()
                else:
                    import shutil
                    shutil.rmtree(dataset_symlink)
            
            try:
                import os
                os.symlink(str(self.yolo_dataset_dir), str(dataset_symlink))
                logger.info(f"Created symlink: {dataset_symlink} -> {self.yolo_dataset_dir}")
                
                # Kh√¥ng copy config v√¨ symlink ƒë√£ tr·ªè ƒë·∫øn dataset g·ªëc
                logger.info("Using symlinked dataset, no need to copy config")
                
            except Exception as e:
                logger.warning(f"Could not create symlink: {e}")
                # Fallback: copy dataset
                import shutil
                if dataset_symlink.exists():
                    shutil.rmtree(dataset_symlink)
                shutil.copytree(str(self.yolo_dataset_dir), str(dataset_symlink))
                logger.info(f"Copied dataset to: {dataset_symlink}")
                
                # Copy file config v√†o th∆∞ m·ª•c YOLOv7 ch·ªâ khi copy dataset
                config_in_yolov7 = yolov7_data_dir / 'yolo_dataset' / 'dataset.yaml'
                if data_yaml.exists():
                    shutil.copy2(str(data_yaml), str(config_in_yolov7))
                    logger.info(f"Copied config to: {config_in_yolov7}")
            
            # Chu·∫©n b·ªã l·ªánh training
            train_cmd = self._prepare_training_command(data_yaml)
            
            # Debug: Ki·ªÉm tra file config
            logger.info(f"Config file: {data_yaml}")
            logger.info(f"Config exists: {data_yaml.exists()}")
            if data_yaml.exists():
                with open(data_yaml, 'r') as f:
                    config_content = f.read()
                    logger.info(f"Config content:\n{config_content}")
                
                # Test ƒë·ªçc YAML -- L·ªói ƒë·ªçc file train v√† val
                import yaml
                try:
                    with open(data_yaml, 'r') as f:
                        test_data_dict = yaml.load(f, Loader=yaml.SafeLoader)
                    logger.info(f"YAML parsed successfully: {test_data_dict}")
                    
                    # Ki·ªÉm tra c√°c ƒë∆∞·ªùng d·∫´n trong data_dict
                    if 'train' in test_data_dict:
                        train_path = Path(test_data_dict['path']) / test_data_dict['train']
                        logger.info(f"Train path: {train_path}")
                        logger.info(f"Train path exists: {train_path.exists()}")
                    
                    if 'val' in test_data_dict:
                        val_path = Path(test_data_dict['path']) / test_data_dict['val']
                        logger.info(f"Val path: {val_path}")
                        logger.info(f"Val path exists: {val_path.exists()}")
                        
                except Exception as e:
                    logger.error(f"Error parsing YAML: {e}")
            
            # Debug: Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n dataset tr∆∞·ªõc khi training
            logger.info("üîç Debug: Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n dataset...")
            import yaml
            with open(data_yaml, 'r') as f:
                data_dict = yaml.load(f, Loader=yaml.SafeLoader)
            
            logger.info(f"Dataset config: {data_dict}")
            
            # Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n validation
            val_path = Path(data_dict['path']) / data_dict['val']
            logger.info(f"Validation path: {val_path}")
            logger.info(f"Validation path exists: {val_path.exists()}")
            logger.info(f"Validation path absolute: {val_path.resolve()}")
            
            # Ki·ªÉm tra t·ª´ th∆∞ m·ª•c YOLOv7
            yolov7_val_path = yolov7_path / data_dict['path'] / data_dict['val']
            logger.info(f"YOLOv7 validation path: {yolov7_val_path}")
            logger.info(f"YOLOv7 validation path exists: {yolov7_val_path.exists()}")
            logger.info(f"YOLOv7 validation path absolute: {yolov7_val_path.resolve()}")
            
            # Debug th√™m: Ki·ªÉm tra ch√≠nh x√°c nh∆∞ YOLOv7 check_dataset
            logger.info("üîç Debug: Ki·ªÉm tra nh∆∞ YOLOv7 check_dataset...")
            val = data_dict.get('val')
            logger.info(f"Val from config: {val}")
            
            if val and len(val):
                val_paths = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]
                logger.info(f"Val paths after resolve: {val_paths}")
                
                for i, path in enumerate(val_paths):
                    logger.info(f"Val path {i}: {path}")
                    logger.info(f"Val path {i} exists: {path.exists()}")
                    logger.info(f"Val path {i} absolute: {path.resolve()}")
                
                missing_paths = [str(x) for x in val_paths if not x.exists()]
                logger.info(f"Missing paths: {missing_paths}")
                
                if missing_paths:
                    logger.error(f"‚ùå Dataset not found! Missing: {missing_paths}")
                else:
                    logger.info("‚úÖ All validation paths exist!")
            
            # Ch·∫°y training
            logger.info(f"Ch·∫°y l·ªánh training: {train_cmd}")
            
            import subprocess
            import sys
            
            # S·ª≠ d·ª•ng Python interpreter hi·ªán t·∫°i thay v√¨ python trong th∆∞ m·ª•c YOLOv7
            python_cmd = sys.executable
            train_cmd = train_cmd.replace("python train.py", f"{python_cmd} train.py")
            
            # Th√™m YOLOv7 path v√†o PYTHONPATH
            env = os.environ.copy()
            env['PYTHONPATH'] = f"{yolov7_path}:{env.get('PYTHONPATH', '')}"
            
            result = subprocess.run(
                train_cmd,
                shell=True,
                capture_output=True,
                text=True,
                cwd=yolov7_path,
                env=env
            )
            
            if result.returncode == 0:
                logger.info("Hu·∫•n luy·ªán YOLOv7 th√†nh c√¥ng!")
                return True
            else:
                logger.error("L·ªói trong qu√° tr√¨nh training:")
                logger.error(result.stderr)
                return False
                
        except Exception as e:
            logger.error(f"L·ªói khi hu·∫•n luy·ªán YOLOv7: {e}")
            return False
    

    
    def _install_yolov7_dependencies(self, yolov7_path: Path) -> bool:
        """
        C√†i ƒë·∫∑t dependencies cho YOLOv7
        
        Args:
            yolov7_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c YOLOv7
            
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            logger.info("C√†i ƒë·∫∑t YOLOv7 dependencies...")
            
            # Ki·ªÉm tra file requirements.txt
            requirements_file = yolov7_path / 'requirements.txt'
            if not requirements_file.exists():
                logger.warning("Kh√¥ng t√¨m th·∫•y requirements.txt trong YOLOv7")
                return False
            
            # C√†i ƒë·∫∑t dependencies
            import subprocess
            result = subprocess.run(
                f"pip install -r {requirements_file}",
                shell=True,
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                logger.info("‚úÖ C√†i ƒë·∫∑t YOLOv7 dependencies th√†nh c√¥ng")
                return True
            else:
                logger.warning(f"‚ö†Ô∏è L·ªói c√†i ƒë·∫∑t dependencies: {result.stderr}")
                return False
                
        except Exception as e:
            logger.error(f"L·ªói khi c√†i ƒë·∫∑t YOLOv7 dependencies: {e}")
            return False
    
    def _create_yolo_data_config(self) -> Optional[Path]:
        """
        T·∫°o file config cho YOLOv7 training
        
        Returns:
            Path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file config
        """
        try:
            # ƒê·∫øm s·ªë l∆∞·ª£ng ·∫£nh trong t·ª´ng split
            train_dir = self.yolo_dataset_dir / 'images' / 'train'
            val_dir = self.yolo_dataset_dir / 'images' / 'val'
            
            train_count = len(list(train_dir.glob('*.jpg'))) + len(list(train_dir.glob('*.png')))
            val_count = len(list(val_dir.glob('*.jpg'))) + len(list(val_dir.glob('*.png')))
            
            # T·∫°o n·ªôi dung config v·ªõi ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·ªÉ tr√°nh l·ªói resolve
            # Trong Docker container, yolov7 n·∫±m ·ªü /app/yolov7 v√† dataset ·ªü /app/data/yolo_dataset
            yolov7_path = Path(__file__).parent.parent / 'yolov7'
            
            # Ki·ªÉm tra xem c√≥ ƒëang ch·∫°y trong Docker container kh√¥ng
            if Path("/app").exists():
                # Trong Docker container - s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
                absolute_path = "/app/data/yolo_dataset"
                logger.info(f"Running in Docker container, using absolute path: {absolute_path}")
            else:
                # Trong m√¥i tr∆∞·ªùng local - s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
                absolute_path = str(self.yolo_dataset_dir.absolute())
                logger.info(f"Running in local environment, using absolute path: {absolute_path}")
            
            # Debug: Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n cu·ªëi c√πng
            final_path = Path(absolute_path)
            logger.info(f"Final dataset path: {final_path}")
            logger.info(f"Final dataset path exists: {final_path.exists()}")
            
            if final_path.exists():
                train_final = final_path / "images" / "train"
                val_final = final_path / "images" / "val"
                logger.info(f"Final train path: {train_final}")
                logger.info(f"Final train path exists: {train_final.exists()}")
                logger.info(f"Final val path: {val_final}")
                logger.info(f"Final val path exists: {val_final.exists()}")
            else:
                logger.error(f"Final dataset path does not exist: {final_path}")
            
            config_content = f"""
# YOLOv7 Face Detection Dataset Config
path: {absolute_path}  # dataset root dir (absolute path)
train: {absolute_path}/images/train  # train images (absolute path)
val: {absolute_path}/images/val  # val images (absolute path)

# Classes
nc: 1  # number of classes
names: ['face']  # class names

# Dataset info
train_count: {train_count}
val_count: {val_count}
total_count: {train_count + val_count}
"""
            
            # L∆∞u config
            config_file = self.yolo_dataset_dir / 'dataset.yaml'
            with open(config_file, 'w') as f:
                f.write(config_content)
            
            logger.info(f"T·∫°o YOLO config t·∫°i: {config_file}")
            return config_file
            
        except Exception as e:
            logger.error(f"L·ªói khi t·∫°o YOLO config: {e}")
            return None
    
    def _validate_dataset(self) -> bool:
        """
        Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa dataset
        
        Returns:
            bool: True n·∫øu dataset h·ª£p l·ªá
        """
        try:
            logger.info("Ki·ªÉm tra dataset...")
            
            # Ki·ªÉm tra th∆∞ m·ª•c images
            train_images_dir = self.yolo_dataset_dir / 'images' / 'train'
            val_images_dir = self.yolo_dataset_dir / 'images' / 'val'
            
            if not train_images_dir.exists():
                logger.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c train images: {train_images_dir}")
                return False
            
            if not val_images_dir.exists():
                logger.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c val images: {val_images_dir}")
                return False
            
            # Ki·ªÉm tra th∆∞ m·ª•c labels
            train_labels_dir = self.yolo_dataset_dir / 'labels' / 'train'
            val_labels_dir = self.yolo_dataset_dir / 'labels' / 'val'
            
            if not train_labels_dir.exists():
                logger.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c train labels: {train_labels_dir}")
                return False
            
            if not val_labels_dir.exists():
                logger.error(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c val labels: {val_labels_dir}")
                return False
            
            # ƒê·∫øm s·ªë l∆∞·ª£ng ·∫£nh v√† labels
            train_images = list(train_images_dir.glob('*.jpg')) + list(train_images_dir.glob('*.png'))
            val_images = list(val_images_dir.glob('*.jpg')) + list(val_images_dir.glob('*.png'))
            
            train_labels = list(train_labels_dir.glob('*.txt'))
            val_labels = list(val_labels_dir.glob('*.txt'))
            
            logger.info(f"Train images: {len(train_images)}, Train labels: {len(train_labels)}")
            logger.info(f"Val images: {len(val_images)}, Val labels: {len(val_labels)}")
            
            # Ki·ªÉm tra xem c√≥ √≠t nh·∫•t 1 ·∫£nh trong m·ªói split kh√¥ng
            if len(train_images) == 0:
                logger.error("Kh√¥ng c√≥ ·∫£nh training")
                return False
            
            if len(val_images) == 0:
                logger.error("Kh√¥ng c√≥ ·∫£nh validation")
                return False
            
            # Ki·ªÉm tra xem s·ªë l∆∞·ª£ng ·∫£nh v√† labels c√≥ kh·ªõp kh√¥ng
            if len(train_images) != len(train_labels):
                logger.warning(f"S·ªë l∆∞·ª£ng train images ({len(train_images)}) kh√¥ng kh·ªõp v·ªõi train labels ({len(train_labels)})")
            
            if len(val_images) != len(val_labels):
                logger.warning(f"S·ªë l∆∞·ª£ng val images ({len(val_images)}) kh√¥ng kh·ªõp v·ªõi val labels ({len(val_labels)})")
            
            logger.info("‚úÖ Dataset validation th√†nh c√¥ng")
            return True
            
        except Exception as e:
            logger.error(f"L·ªói khi validate dataset: {e}")
            return False
    
    def _prepare_training_command(self, data_yaml: Path) -> str:
        """
        Chu·∫©n b·ªã l·ªánh training YOLOv7
        
        Args:
            data_yaml: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file config
            
        Returns:
            str: L·ªánh training
        """
        # S·ª≠ d·ª•ng YOLOv7 tiny ƒë·ªÉ training nhanh h∆°n
        # S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ƒë·ªÉ tr√°nh l·ªói resolve
        if Path("/app").exists():
            # Trong Docker container - s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
            absolute_data_yaml = "/app/data/yolo_dataset/dataset.yaml"
            model_config = "cfg/training/yolov7-tiny.yaml"
            project_path = "/app/models/trained"
        else:
            # Trong m√¥i tr∆∞·ªùng local - s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
            absolute_data_yaml = str(data_yaml.absolute())
            model_config = "cfg/training/yolov7-tiny.yaml"
            project_path = str(Path(__file__).parent.parent / "models/trained")
        
        cmd_parts = [
            "python train.py",
            f"--data {absolute_data_yaml}",
            f"--cfg {model_config}",
            "--weights ''",  # Kh√¥ng s·ª≠ d·ª•ng pretrained weights ƒë·ªÉ tr√°nh l·ªói download
            f"--epochs {TRAINING_CONFIG['epochs']}",
            f"--batch-size {TRAINING_CONFIG['batch_size']}",
            f"--img-size {TRAINING_CONFIG['img_size']}",
            "--device 0" if TRAINING_CONFIG.get('use_gpu', False) else "--device cpu",
            f"--project {project_path}",
            "--name face_detection",
            "--exist-ok",
            "--workers 0"  # Gi·∫£m workers ƒë·ªÉ tr√°nh l·ªói shared memory
        ]
        
        final_cmd = " ".join(cmd_parts)
        logger.info(f"üîß Generated training command: {final_cmd}")
        return final_cmd
    
    def save_training_log(self, training_info: Dict):
        """
        L∆∞u th√¥ng tin training
        
        Args:
            training_info: Th√¥ng tin training
        """
        try:
            log_file = Path(TRAINING_CONFIG['logging']['file']).parent / 'training_log.txt'
            
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write("=== FACE RECOGNITION TRAINING LOG ===\n")
                f.write(f"Training time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Dataset prepared: {training_info.get('dataset_prepared', False)}\n")
                f.write(f"Embeddings created: {training_info.get('embeddings_created', False)}\n")
                f.write(f"YOLOv7 trained: {training_info.get('yolov7_trained', False)}\n")
                f.write(f"Total faces processed: {training_info.get('total_faces', 0)}\n")
                f.write(f"Successful embeddings: {training_info.get('successful_embeddings', 0)}\n")
                f.write("=" * 40 + "\n")
            
            logger.info(f"L∆∞u training log t·∫°i: {log_file}")
            
        except Exception as e:
            logger.error(f"L·ªói khi l∆∞u training log: {e}")
    
    def run_training_pipeline(self) -> bool:
        """
        Ch·∫°y to√†n b·ªô pipeline training
        
        Returns:
            bool: True n·∫øu th√†nh c√¥ng
        """
        try:
            logger.info("B·∫Øt ƒë·∫ßu pipeline training...")
            start_time = time.time()
            
            training_info = {
                'dataset_prepared': False,
                'embeddings_created': False,
                'yolov7_trained': False,
                'total_faces': 0,
                'successful_embeddings': 0
            }
            
            # B∆∞·ªõc 1: Chu·∫©n b·ªã dataset
            logger.info("=== B∆Ø·ªöC 1: Chu·∫©n b·ªã dataset ===")
            if self.prepare_dataset():
                training_info['dataset_prepared'] = True
                logger.info("‚úÖ Chu·∫©n b·ªã dataset th√†nh c√¥ng")
            else:
                logger.error("‚ùå Chu·∫©n b·ªã dataset th·∫•t b·∫°i")
                return False
            
            # B∆∞·ªõc 2: T·∫°o embeddings
            logger.info("=== B∆Ø·ªöC 2: T·∫°o embeddings ===")
            if self.create_embeddings():
                training_info['embeddings_created'] = True
                logger.info("‚úÖ T·∫°o embeddings th√†nh c√¥ng")
            else:
                logger.error("‚ùå T·∫°o embeddings th·∫•t b·∫°i")
                return False
            
            # B∆∞·ªõc 3: Hu·∫•n luy·ªán YOLOv7
            logger.info("=== B∆Ø·ªöC 3: Hu·∫•n luy·ªán YOLOv7 ===")
            if self.train_yolov7():
                training_info['yolov7_trained'] = True
                logger.info("‚úÖ Hu·∫•n luy·ªán YOLOv7 th√†nh c√¥ng")
            else:
                logger.warning("‚ö†Ô∏è Hu·∫•n luy·ªán YOLOv7 th·∫•t b·∫°i (c√≥ th·ªÉ s·ª≠ d·ª•ng pretrained)")
            
            # L∆∞u training log
            training_time = time.time() - start_time
            training_info['training_time'] = training_time
            self.save_training_log(training_info)
            
            logger.info(f"Pipeline training ho√†n th√†nh trong {training_time:.2f} gi√¢y")
            return True
            
        except Exception as e:
            logger.error(f"L·ªói trong training pipeline: {e}")
            return False

def main():
    """H√†m main ƒë·ªÉ ch·∫°y training"""
    trainer = FaceRecognitionTrainer()
    
    # Ch·∫°y pipeline training
    success = trainer.run_training_pipeline()
    
    if success:
        logger.info("üéâ Training ho√†n th√†nh th√†nh c√¥ng!")
        print("Training ho√†n th√†nh th√†nh c√¥ng!")
    else:
        logger.error("‚ùå Training th·∫•t b·∫°i!")
        print("Training th·∫•t b·∫°i!")

if __name__ == '__main__':
    main() 